{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3be7535e-bf5a-41d5-838c-3def0e384173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "from captum.attr import IntegratedGradients, DeepLiftShap, GuidedGradCam\n",
    "\n",
    "\n",
    "\n",
    "from captum.attr import DeepLiftShap\n",
    "from captum.attr import visualization as viz\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", Warning)\n",
    "import torch.nn as nn \n",
    "from matplotlib.pyplot import figure\n",
    "pd.set_option('display.max_rows', 5000)\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.pad_inches'] = 0\n",
    "px = 1/plt.rcParams['figure.dpi']  # pixel in inches\n",
    "figure(figsize=(512*px, 512*px), dpi=300)\n",
    "\n",
    "import ast\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from pathlib import Path\n",
    "# import hgs_to_pixel_converter as hgs_to_pix\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbc99817-4487-4326-be26-2366cff7b07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "694df960-b46e-4cd1-9d16-3abc2d62f830",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyJP2Dataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.root_dir, self.annotations.iloc[index, 0])\n",
    "        hmi = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(hmi)\n",
    "            \n",
    "        y_prob = round(float((self.annotations.iloc[index, 1])), 2)\n",
    "        y_label = str(self.annotations.iloc[index, 2])\n",
    "        \n",
    "        return (image, y_prob, y_label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06c2169e-7a48-44e0-b67a-55125dd863cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG16(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU()\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU()\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU()\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU()\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU()\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU()\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU()\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU()\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU()\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU()\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU()\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "model_PATH1 = '../create_models/trained_models/fold1/fold1.pth'\n",
    "weights1 = torch.load(model_PATH1)\n",
    "test_model = VGG16().to(device)\n",
    "test_model.load_state_dict(weights1['model_state_dict'])\n",
    "test_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f6b206f-b1e3-48ca-a200-f83d24866bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>flare_prob</th>\n",
       "      <th>goes_class</th>\n",
       "      <th>fl_location</th>\n",
       "      <th>flare_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014/02/24/HMI.m2014.02.24_01.00.00.jpg</td>\n",
       "      <td>0.807306</td>\n",
       "      <td>X4.9</td>\n",
       "      <td>(-82, -12)</td>\n",
       "      <td>2014-02-25 00:39:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 timestamp  flare_prob goes_class fl_location  \\\n",
       "0  2014/02/24/HMI.m2014.02.24_01.00.00.jpg    0.807306       X4.9  (-82, -12)   \n",
       "\n",
       "           flare_start  \n",
       "0  2014-02-25 00:39:00  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_file = 'x_class.csv'\n",
    "data_path = '/scratch/cpandey1/hmi_jpgs_512/'\n",
    "data_transforms = Compose([ToTensor()])\n",
    "dataset1 = MyJP2Dataset(csv_file = csv_file , \n",
    "                             root_dir = data_path,\n",
    "                             transform = data_transforms)\n",
    "batch_size = 164\n",
    "loader1 = DataLoader(dataset=dataset1, batch_size=batch_size, shuffle = False, num_workers=8)\n",
    "df = pd.read_csv(csv_file)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1fd1eb7-8328-4f24-bc91-6b137212c9fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8600, dtype=torch.float64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def imshow(img, transpose = True):\n",
    "#     img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "dataiter = iter(loader1)\n",
    "images, probs, labels = next(dataiter)\n",
    "probs[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9e024a8-da15-4f83-bc52-3470d5007b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attribute_image_features(algorithm, input, target, **kwargs):\n",
    "    test_model.zero_grad()\n",
    "    tensor_attributions = algorithm.attribute(input,\n",
    "                                              target=target,\n",
    "                                              **kwargs\n",
    "                               )\n",
    "    return tensor_attributions\n",
    "\n",
    "def plot_attributions_guided_grad_cam(img, filename_grad, target):\n",
    "    dpi=300\n",
    "    plt.rcParams['figure.dpi'] = dpi\n",
    "    plt.rcParams['savefig.pad_inches'] = 0\n",
    "    px = 1/plt.rcParams['figure.dpi']  # pixel in inches\n",
    "    figure(dpi=dpi)\n",
    "    filename = filename_grad\n",
    "    inp = img.unsqueeze(0)\n",
    "    inp.requires_grad = True\n",
    "    guided_gc = GuidedGradCam(test_model, test_model.features[28])\n",
    "    grads = guided_gc.attribute(inp.to(device), target=target)\n",
    "    grads = np.transpose(grads.squeeze(0).cpu().detach().numpy(), (1, 2, 0))\n",
    "    original_image = np.transpose((img.cpu().detach().numpy() / 2) + 0.5, (1, 2, 0))\n",
    "    \n",
    "    fig, ax = viz.visualize_image_attr(grads, method=\"heat_map\",sign=\"absolute_value\",alpha_overlay=1.0,\n",
    "                              show_colorbar=True)\n",
    "    # Define custom colormap\n",
    "    # cmap=plt.cm.Blues\n",
    "    \n",
    "    # fig, ax = viz.visualize_image_attr(grads, original_image, method=\"blended_heat_map\", sign=\"absolute_value\",alpha_overlay=0.8,\n",
    "    #                           show_colorbar=True)\n",
    "    \n",
    "    # cbar = ax.figure.colorbar(ax.images[1])\n",
    "    # cbar.ax.set_position([0.85, 0.15, 0.05, 0.7])\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    fig.tight_layout(pad=0.05)\n",
    "    fig.savefig(f'{filename}.svg', dpi=dpi)\n",
    "    # fig.savefig(f'{filename}.png', dpi=dpi)\n",
    "    \n",
    "    \n",
    "def plot_attributions_deepshap(img, filename_shap, i, target):\n",
    "    dpi=300\n",
    "    plt.rcParams['figure.dpi'] = dpi\n",
    "    plt.rcParams['savefig.pad_inches'] = 0\n",
    "    px = 1/plt.rcParams['figure.dpi']  # pixel in inches\n",
    "    figure(dpi=dpi)\n",
    "    filename = filename_shap\n",
    "    inp = img.unsqueeze(0)\n",
    "    inp.requires_grad = True\n",
    "    saliency = DeepLiftShap(test_model)\n",
    "    if i<10:\n",
    "        grads = saliency.attribute(inp.to(device), baselines= images[i:i+9].to(device), target=target)\n",
    "    else:\n",
    "        grads = saliency.attribute(inp.to(device), baselines= images[i-5:i+5].to(device), target=target)\n",
    "    grads = np.transpose(grads.squeeze(0).cpu().detach().numpy(), (1, 2, 0))\n",
    "    original_image = np.transpose((img.cpu().detach().numpy() / 2) + 0.5, (1, 2, 0))\n",
    "    fig, ax = viz.visualize_image_attr(grads, method=\"heat_map\",sign=\"absolute_value\",alpha_overlay=1.0,\n",
    "                              show_colorbar=True)\n",
    "    # fig, ax = viz.visualize_image_attr(grads, original_image, method=\"blended_heat_map\", sign=\"absolute_value\",alpha_overlay=0.8,\n",
    "    #                           show_colorbar=True)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    fig.tight_layout(pad=0.05)\n",
    "    fig.savefig(f'{filename}.svg', dpi=dpi)\n",
    "    \n",
    "    \n",
    "def plot_attributions_intgrad(img, filename_intgrad, target):\n",
    "    dpi=300\n",
    "    plt.rcParams['figure.dpi'] = dpi\n",
    "    plt.rcParams['savefig.pad_inches'] = 0\n",
    "    px = 1/plt.rcParams['figure.dpi']  # pixel in inches\n",
    "    figure(dpi=dpi)\n",
    "    filename = filename_intgrad\n",
    "    inp = img.unsqueeze(0)\n",
    "    inp.requires_grad = True\n",
    "    inp=inp.to(device)\n",
    "    ig = IntegratedGradients(test_model)\n",
    "    attr_ig, delta = attribute_image_features(ig, inp, target, baselines=inp * 0 + 0.5, return_convergence_delta=True)\n",
    "    attr_ig = np.transpose(attr_ig.squeeze(0).cpu().detach().numpy(), (1, 2, 0))\n",
    "    original_image = np.transpose((img.cpu().detach().numpy() / 2) + 0.5, (1, 2, 0))\n",
    "    fig, ax = viz.visualize_image_attr(attr_ig, method=\"heat_map\",sign=\"absolute_value\",alpha_overlay=1.0,\n",
    "                              show_colorbar=True)\n",
    "#     print('Approximation delta: ', abs(delta))\n",
    "    # fig, ax = viz.visualize_image_attr(attr_ig, original_image, method=\"blended_heat_map\",sign=\"absolute_value\",alpha_overlay=0.8,\n",
    "    #                           show_colorbar=True)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    fig.tight_layout(pad=0.05)\n",
    "    fig.savefig(f'{filename}.svg', dpi=dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "876b8b90-73a9-4055-87bb-1fbcdf3d2305",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)): \n",
    "    pth_grad = 'gradcam/' + df.loc[i]['timestamp'][0:11]\n",
    "    Path(pth_grad).mkdir(parents=True, exist_ok=True)\n",
    "    img = images[i]\n",
    "    filename_grad = 'gradcam/' + df.loc[i]['timestamp'][:-4]\n",
    "    plot_attributions_guided_grad_cam(img, filename_grad, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c9b1d71-78da-4d88-9f42-dc9405d8e309",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(df)): \n",
    "    pth_shap = 'shap/' + df.loc[i]['timestamp'][0:11]\n",
    "    Path(pth_shap).mkdir(parents=True, exist_ok=True)\n",
    "    img = images[i]\n",
    "    filename_shap = 'shap/' + df.loc[i]['timestamp'][:-4]\n",
    "    plot_attributions_deepshap(img, filename_shap, i, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eabb6285-e074-45a7-b16e-c765e1264dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(df)): \n",
    "    pth_ig = 'intgradtest/' + df.loc[i]['timestamp'][0:11]\n",
    "    Path(pth_ig).mkdir(parents=True, exist_ok=True)\n",
    "    img = images[i]\n",
    "    filename_intgrad = 'intgradtest/' + df.loc[i]['timestamp'][:-4]\n",
    "    plot_attributions_intgrad(img, filename_intgrad, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
