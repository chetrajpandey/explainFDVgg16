{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "410bafd4-9cbd-4a7b-a3a2-d13ba68307f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from PIL import Image\n",
    "from model import VGG16\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from torch.nn.functional import softmax\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", Warning)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datetime import timedelta\n",
    "# from evaluation import sklearn_Compatible_preds_and_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eddf6202-fe0e-40ec-ac53-44d3c61631c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyJP2Dataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.root_dir, self.annotations.iloc[index, 0])\n",
    "        hmi = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(hmi)\n",
    "            \n",
    "        y_label = torch.tensor(int(self.annotations.iloc[index, 1]))\n",
    "        \n",
    "        return (image, y_label, img_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b6a7a4c-04ed-4951-b2c9-5508db8895df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "im_size = 512\n",
    "datapath = '/scratch/cpandey1/hmi_jpgs_512/'\n",
    "partition1_path = '../data_labeling/data_labels/Fold1_val.csv'\n",
    "partition2_path = '../data_labeling/data_labels/Fold2_val.csv'\n",
    "partition3_path = '../data_labeling/data_labels/Fold3_val.csv'\n",
    "partition4_path = '../data_labeling/data_labels/Fold4_val.csv'\n",
    "\n",
    "\n",
    "transformations = Compose([\n",
    "    Resize(im_size),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "part1 = MyJP2Dataset(csv_file = partition1_path, \n",
    "                             root_dir = datapath,\n",
    "                             transform = transformations)\n",
    "part2 = MyJP2Dataset(csv_file = partition2_path, \n",
    "                             root_dir = datapath,\n",
    "                             transform = transformations)\n",
    "part3 = MyJP2Dataset(csv_file = partition3_path, \n",
    "                             root_dir = datapath,\n",
    "                             transform = transformations)\n",
    "part4 = MyJP2Dataset(csv_file = partition4_path, \n",
    "                             root_dir = datapath,\n",
    "                             transform = transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db6f87a6-ec32-4284-a101-844d1bb72fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "part1_loader = DataLoader(dataset=part1, batch_size=48, num_workers=4, shuffle=False)\n",
    "part2_loader = DataLoader(dataset=part2, batch_size=48, num_workers=4, shuffle=False)\n",
    "part3_loader = DataLoader(dataset=part3, batch_size=48, num_workers=4, shuffle=False)\n",
    "part4_loader = DataLoader(dataset=part4, batch_size=48, num_workers=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee167eb3-1e70-4cdf-9ca5-22dcd43f5257",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "model_PATH1 = '../create_models/trained_models/fold1/fold1.pth'\n",
    "model_PATH2 = '../create_models/trained_models/fold2/fold2.pth'\n",
    "model_PATH3 = '../create_models/trained_models/fold3/fold3.pth'\n",
    "model_PATH4 = '../create_models/trained_models/fold4/fold4.pth'\n",
    "weights1 = torch.load(model_PATH1)\n",
    "weights2 = torch.load(model_PATH2)\n",
    "weights3 = torch.load(model_PATH3)\n",
    "weights4 = torch.load(model_PATH4)\n",
    "test_model = VGG16(ipt_size=(512, 512), train=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6019d03c-aab0-43d1-8c6a-7766bdd92bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_Compatible_preds_and_targets(model_prediction_list, model_target_list, model_path_list):\n",
    "    y_pred_list = []\n",
    "    preds = []\n",
    "    target_list = []\n",
    "    tgts = []\n",
    "    path_list = []\n",
    "    path = []\n",
    "    y_pred_list = [a.squeeze().tolist() for a in model_prediction_list]\n",
    "    preds = [item for sublist in y_pred_list for item in sublist]\n",
    "    target_list = [a.squeeze().tolist() for a in model_target_list]\n",
    "    tgts = [item for sublist in target_list for item in sublist]\n",
    "    path_list = [a for a in model_path_list]\n",
    "    path = [item for sublist in path_list for item in sublist]\n",
    "    return preds,tgts, path\n",
    "\n",
    "\n",
    "def accuracy_score(prediction, target):\n",
    "    TN, FP, FN, TP = confusion_matrix(target, prediction).ravel()\n",
    "    print(\"TP: \", TP, \"FP: \", FP, \"TN: \", TN, \"FN: \", FN)\n",
    "    #TSS Computation also known as \"recall\"\n",
    "    tp_rate = TP / float(TP + FN) if TP > 0 else 0  \n",
    "    fp_rate = FP / float(FP + TN) if FP > 0 else 0\n",
    "    TSS = tp_rate - fp_rate\n",
    "    \n",
    "    #HSS2 Computation\n",
    "    N = TN + FP\n",
    "    P = TP + FN\n",
    "    HSS = (2 * (TP * TN - FN * FP)) / float((P * (FN + TN) + (TP + FP) * N))\n",
    "\n",
    "    return TSS, HSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8e3e4b5-c830-4606-b70d-eaa5dbfb1e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(checkpoint, test_loader, desc ):\n",
    "    test_target_list=[]\n",
    "    test_prediction_list=[]\n",
    "    test_path_list = []\n",
    "    test_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    test_model.eval()\n",
    "    print('***********************', desc, '*************************')\n",
    "    with torch.no_grad():\n",
    "        for d, t, path in test_loader:\n",
    "            # Get data to cuda if possible\n",
    "            d = d.to(device=device)\n",
    "            t = t.to(device=device)\n",
    "    #         pa = path.to(device=device)\n",
    "            test_target_list.append(t)\n",
    "            test_path_list.append(list(path))\n",
    "    #         print(list(path))\n",
    "            # forward pass\n",
    "            s = test_model(d)\n",
    "            #print(\"scores\", s)\n",
    "\n",
    "            # validation batch loss and accuracy\n",
    "    #         l = criterion(s, t)\n",
    "            p = softmax(s,dim=1)\n",
    "    #         print(p[:,1])\n",
    "            test_prediction_list.append(p[:,1])\n",
    "            # accumulating the val_loss and accuracy\n",
    "    #         val_loss += l.item()\n",
    "            #val_acc += acc.item()\n",
    "            del d,t,s,p\n",
    "    a, b, c = sklearn_Compatible_preds_and_targets(test_prediction_list, test_target_list, test_path_list)\n",
    "    preds = [int(i >=0.5) for i in a]\n",
    "    print(accuracy_score(preds, b))\n",
    "    prob_list = pd.DataFrame(\n",
    "    {'timestamp': c,\n",
    "     'flare_prob': a,\n",
    "     'target': b\n",
    "    })\n",
    "\n",
    "    print(prob_list['target'].value_counts())\n",
    "#     prob_list['timestamp'] = prob_list['timestamp'].apply(lambda row: row[35:-4])\n",
    "#     prob_list['timestamp'] = pd.to_datetime(prob_list['timestamp'], format='%Y.%m.%d_%H.%M.%S')\n",
    "    return prob_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8d33563-d191-4557-93ac-b9664efffc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************** Fold-1 Results *************************\n",
      "TP:  1777 FP:  2319 TN:  10135 FN:  557\n",
      "(0.5751486636202545, 0.44014573119868605)\n",
      "0    12454\n",
      "1     2334\n",
      "Name: target, dtype: int64\n",
      "*********************** Fold-2 Results *************************\n",
      "TP:  1237 FP:  3278 TN:  10577 FN:  375\n",
      "(0.5307764394253492, 0.295586748222468)\n",
      "0    13855\n",
      "1     1612\n",
      "Name: target, dtype: int64\n",
      "*********************** Fold-3 Results *************************\n",
      "TP:  1335 FP:  2581 TN:  11727 FN:  1029\n",
      "(0.3843322183890593, 0.30166847919515566)\n",
      "0    14308\n",
      "1     2364\n",
      "Name: target, dtype: int64\n",
      "*********************** Fold-4 Results *************************\n",
      "TP:  2073 FP:  3373 TN:  10659 FN:  617\n",
      "(0.530252836850873, 0.3749851523238552)\n",
      "0    14032\n",
      "1     2690\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "fold1 = predict(weights1, part1_loader, 'Fold-1 Results')\n",
    "fold2 = predict(weights2, part2_loader, 'Fold-2 Results')\n",
    "fold3 = predict(weights3, part3_loader, 'Fold-3 Results')\n",
    "fold4 = predict(weights4, part4_loader, 'Fold-4 Results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcf00a6f-9f33-425f-8509-212d5a2df020",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold1.to_csv(r'../create_models/prediction_results/fold1_res.csv', index=False, header=True, columns=['timestamp', 'flare_prob', 'target'])\n",
    "fold2.to_csv(r'../create_models/prediction_results/fold2_res.csv', index=False, header=True, columns=['timestamp', 'flare_prob', 'target'])\n",
    "fold3.to_csv(r'../create_models/prediction_results/fold3_res.csv', index=False, header=True, columns=['timestamp', 'flare_prob', 'target'])\n",
    "fold4.to_csv(r'../create_models/prediction_results/fold4_res.csv', index=False, header=True, columns=['timestamp', 'flare_prob', 'target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7332947-4c55-4c36-98de-d351c99ff8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold1_val = pd.read_csv(r'../create_models/prediction_results/fold1_res.csv')\n",
    "fold2_val = pd.read_csv(r'../create_models/prediction_results/fold2_res.csv')\n",
    "fold3_val = pd.read_csv(r'../create_models/prediction_results/fold3_res.csv')\n",
    "fold4_val = pd.read_csv(r'../create_models/prediction_results/fold4_res.csv')\n",
    "total = pd.concat([fold1_val, fold2_val, fold3_val, fold4_val])\n",
    "total['timestamp'] = total['timestamp'].apply(lambda row: row[47:-4])\n",
    "total['timestamp'] =  pd.to_datetime(total['timestamp'], format='%Y.%m.%d_%H.%M.%S')\n",
    "total.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c50bf9c8-ac1d-4bfc-83ce-4c44a7ce6949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>flare_prob</th>\n",
       "      <th>target</th>\n",
       "      <th>goes_class</th>\n",
       "      <th>fl_location</th>\n",
       "      <th>flare_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-01 00:00:00</td>\n",
       "      <td>0.171418</td>\n",
       "      <td>0</td>\n",
       "      <td>B8.3</td>\n",
       "      <td>(-56, 30)</td>\n",
       "      <td>2011-01-01 21:52:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2011-01-01 01:00:00</td>\n",
       "      <td>0.213330</td>\n",
       "      <td>0</td>\n",
       "      <td>B8.3</td>\n",
       "      <td>(-56, 30)</td>\n",
       "      <td>2011-01-01 21:52:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2011-01-01 02:00:00</td>\n",
       "      <td>0.201592</td>\n",
       "      <td>0</td>\n",
       "      <td>B8.3</td>\n",
       "      <td>(-56, 30)</td>\n",
       "      <td>2011-01-01 21:52:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2011-01-01 03:00:00</td>\n",
       "      <td>0.237191</td>\n",
       "      <td>0</td>\n",
       "      <td>B8.3</td>\n",
       "      <td>(-56, 30)</td>\n",
       "      <td>2011-01-01 21:52:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2011-01-01 04:00:00</td>\n",
       "      <td>0.204492</td>\n",
       "      <td>0</td>\n",
       "      <td>B8.3</td>\n",
       "      <td>(-56, 30)</td>\n",
       "      <td>2011-01-01 21:52:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63644</th>\n",
       "      <td>16717</td>\n",
       "      <td>2018-12-30 19:00:00</td>\n",
       "      <td>0.018860</td>\n",
       "      <td>0</td>\n",
       "      <td>NF</td>\n",
       "      <td>unk</td>\n",
       "      <td>unk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63645</th>\n",
       "      <td>16718</td>\n",
       "      <td>2018-12-30 20:00:00</td>\n",
       "      <td>0.019474</td>\n",
       "      <td>0</td>\n",
       "      <td>NF</td>\n",
       "      <td>unk</td>\n",
       "      <td>unk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63646</th>\n",
       "      <td>16719</td>\n",
       "      <td>2018-12-30 21:00:00</td>\n",
       "      <td>0.019670</td>\n",
       "      <td>0</td>\n",
       "      <td>NF</td>\n",
       "      <td>unk</td>\n",
       "      <td>unk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63647</th>\n",
       "      <td>16720</td>\n",
       "      <td>2018-12-30 22:00:00</td>\n",
       "      <td>0.019339</td>\n",
       "      <td>0</td>\n",
       "      <td>NF</td>\n",
       "      <td>unk</td>\n",
       "      <td>unk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63648</th>\n",
       "      <td>16721</td>\n",
       "      <td>2018-12-30 23:00:00</td>\n",
       "      <td>0.020211</td>\n",
       "      <td>0</td>\n",
       "      <td>NF</td>\n",
       "      <td>unk</td>\n",
       "      <td>unk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63649 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index           timestamp  flare_prob  target goes_class fl_location  \\\n",
       "0          0 2011-01-01 00:00:00    0.171418       0       B8.3   (-56, 30)   \n",
       "1          1 2011-01-01 01:00:00    0.213330       0       B8.3   (-56, 30)   \n",
       "2          2 2011-01-01 02:00:00    0.201592       0       B8.3   (-56, 30)   \n",
       "3          3 2011-01-01 03:00:00    0.237191       0       B8.3   (-56, 30)   \n",
       "4          4 2011-01-01 04:00:00    0.204492       0       B8.3   (-56, 30)   \n",
       "...      ...                 ...         ...     ...        ...         ...   \n",
       "63644  16717 2018-12-30 19:00:00    0.018860       0         NF         unk   \n",
       "63645  16718 2018-12-30 20:00:00    0.019474       0         NF         unk   \n",
       "63646  16719 2018-12-30 21:00:00    0.019670       0         NF         unk   \n",
       "63647  16720 2018-12-30 22:00:00    0.019339       0         NF         unk   \n",
       "63648  16721 2018-12-30 23:00:00    0.020211       0         NF         unk   \n",
       "\n",
       "               flare_start  \n",
       "0      2011-01-01 21:52:00  \n",
       "1      2011-01-01 21:52:00  \n",
       "2      2011-01-01 21:52:00  \n",
       "3      2011-01-01 21:52:00  \n",
       "4      2011-01-01 21:52:00  \n",
       "...                    ...  \n",
       "63644                  unk  \n",
       "63645                  unk  \n",
       "63646                  unk  \n",
       "63647                  unk  \n",
       "63648                  unk  \n",
       "\n",
       "[63649 rows x 7 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "details = pd.read_csv('M_full_dataset_cleaned_1_hours_with_loc_and_time_new.csv')\n",
    "details['timestamp'] = details['label'].apply(lambda row: row[16:-4])\n",
    "details['timestamp'] = pd.to_datetime(details['timestamp'], format='%Y.%m.%d_%H.%M.%S')\n",
    "details.drop(columns=['label'], inplace=True)\n",
    "df = total.merge(details, how='left', on='timestamp')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc8b181b-e4bc-4776-9d35-2a8abc0d022c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_to_filename(df):\n",
    "    cols=['timestamp']\n",
    "    for items in cols:\n",
    "\n",
    "        df[items] = pd.to_datetime(df[items], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        #Renaming label(Date) to this format of file HMI.m2010.05.21_12.00.00 \n",
    "        df[items] = df[items].dt.year.astype(str) + '/' \\\n",
    "            + df[items].dt.month.map(\"{:02}\".format).astype(str) + '/'\\\n",
    "            + df[items].dt.day.map(\"{:02}\".format).astype(str) + '/'+ 'HMI.m'+ df[items].dt.year.astype(str) + '.' \\\n",
    "            + df[items].dt.month.map(\"{:02}\".format).astype(str) + '.'\\\n",
    "            + df[items].dt.day.map(\"{:02}\".format).astype(str) + '_' \\\n",
    "            + df[items].dt.hour.map(\"{:02}\".format).astype(str) + '.'\\\n",
    "            + df[items].dt.minute.map(\"{:02}\".format).astype(str) + '.'\\\n",
    "            + df[items].dt.second.map(\"{:02}\".format).astype(str) + '.jpg'\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09f35a81-166d-47ed-b4ae-140dbcf39e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************X Class flares locations***************\n",
      "Total Instances:  880\n",
      "With in Central Locations\n",
      "TP:  597 FN:  71\n",
      "Beyond Central Locations (Limb Locations)\n",
      "TP:  164 FN:  48 \n",
      "\n",
      "*************M Class flares locations***************\n",
      "Total Instances:  8120\n",
      "With in Central Locations\n",
      "TP:  4464 FN:  1366\n",
      "Beyond Central Locations (Limb Locations)\n",
      "TP:  1197 FN:  1093 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def location_analysis(df, flareclass):\n",
    "    X = df.loc[(df.goes_class.str.startswith(flareclass))].copy()\n",
    "    X[[\"x\", \"y\"]] = X[\"fl_location\"].str.strip(r\"[()]\").str.split(\",\", expand=True).astype(str)\n",
    "    X['x'] = pd.to_numeric(X['x']).round(decimals=2).astype(str).replace(r'\\.0$', '', regex=True)\n",
    "    X[[\"x\", \"y\"]] = X[['x', 'y']].astype(float)\n",
    "    pos = X[(X.flare_prob>=0.5)]\n",
    "    neg = X[(X.flare_prob<0.5)]\n",
    "    pos_limb = len(pos.loc[(pos.x<-70) | (pos.x>70)])+len(pos.loc[(pos.y<-70) | (pos.y>70)])\n",
    "    neg_limb = len(neg.loc[(neg.x<-70) | (neg.x>70)])+ len(neg.loc[(neg.y<-70) | (neg.y>70)])\n",
    "    pos_center = len(pos.loc[(pos.y>=-70) & (pos.y<=70) & (pos.x>=-70) & (pos.x<=70)])\n",
    "    neg_center = len(neg.loc[(neg.y>=-70) & (neg.y<=70) & (neg.x>=-70) & (neg.x<=70)])\n",
    "    print(f'*************{flareclass} Class flares locations***************')\n",
    "    print(\"Total Instances: \", len(X))\n",
    "    print('With in Central Locations')\n",
    "    print('TP: ', pos_center, 'FN: ', neg_center)\n",
    "    print('Beyond Central Locations (Limb Locations)')\n",
    "    print('TP: ', pos_limb, 'FN: ', neg_limb, '\\n')\n",
    "    \n",
    "location_analysis(df.copy(), 'X')\n",
    "location_analysis(df.copy(), 'M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a46d3035-a13e-4ffa-8e31-f8002b379524",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_df = date_to_filename(df)\n",
    "df_x = new_df.loc[(new_df.goes_class.str.startswith('X'))]\n",
    "df_m = new_df.loc[(new_df.goes_class.str.startswith('M'))]\n",
    "df_c = new_df.loc[(new_df.goes_class.str.startswith('C')) & (new_df.flare_prob>=0.8)]\n",
    "cols = ['timestamp', 'flare_prob', 'goes_class', 'fl_location', 'flare_start']\n",
    "df_x.to_csv(r'x_class.csv', index=False, header=True, columns=cols)\n",
    "df_m.to_csv(r'm_class.csv', index=False, header=True, columns=cols)\n",
    "df_c.to_csv(r'c_class.csv', index=False, header=True, columns=cols)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
